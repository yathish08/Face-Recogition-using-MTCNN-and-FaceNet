{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ab7d935-8bee-4aff-a400-56eaf127f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f6d868d-2574-41cd-92ac-3f38c696305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MTCNN for face detection and FaceNet for embedding extraction\n",
    "mtcnn = MTCNN()\n",
    "facenet = InceptionResnetV1(pretrained='vggface2').eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07370380-4d52-4b41-ae69-409a9058681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face embedding for person1 stored successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dictionary to store embeddings\n",
    "known_faces = {}\n",
    "\n",
    "image_path = r\"C:\\Users\\Yathish\\cv\\person1.jpeg\"   # Change this to actual image path\n",
    "image_name = os.path.basename(image_path).split('.')[0]  # Extracts \"person1\" from \"person1.jpeg\"\n",
    "\n",
    "# Load and detect face from the image\n",
    "img = Image.open(image_path)\n",
    "img_cropped = mtcnn(img)\n",
    "\n",
    "if img_cropped is not None:\n",
    "    img_cropped = img_cropped.unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        embedding = facenet(img_cropped)  # Generate embedding\n",
    "\n",
    "    # Store embedding using extracted name\n",
    "    known_faces[image_name] = embedding.numpy()\n",
    "\n",
    "    # Save embeddings to a file\n",
    "    np.save(\"known_faces.npy\", known_faces)\n",
    "    print(f\"Face embedding for {image_name} stored successfully!\")\n",
    "else:\n",
    "    print(\"No face detected in the provided image.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2929d-2b50-493e-88d7-a9a8c02956da",
   "metadata": {},
   "source": [
    "Before running the next cell, update the image path to the person you want to recognize. When that person appears on the webcam, the system will detect them and display \"Hi [Person's Name]!\" on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb34de21-ea65-4d22-bf39-6b2dd91a70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded known faces: dict_keys(['person1'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load stored embeddings\n",
    "try:\n",
    "    known_faces = np.load(\"known_faces.npy\", allow_pickle=True).item()\n",
    "    print(\"Loaded known faces:\", known_faces.keys())\n",
    "except FileNotFoundError:\n",
    "    print(\"No stored faces found! Please store embeddings first.\")\n",
    "    known_faces = {}\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame)\n",
    "\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            x, y, x2, y2 = map(int, box)  # Convert bounding box coordinates to integers\n",
    "\n",
    "            # Extract face ROI\n",
    "            face_roi = frame[y:y2, x:x2]\n",
    "\n",
    "            # Ensure the face ROI is valid before processing\n",
    "            if face_roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            face_roi = cv2.resize(face_roi, (160, 160))  # Resize for FaceNet\n",
    "\n",
    "            # Convert to tensor for FaceNet\n",
    "            face_roi = np.transpose(face_roi, (2, 0, 1))  # Change to (C, H, W)\n",
    "            face_roi = torch.tensor(face_roi, dtype=torch.float32).unsqueeze(0) / 255.0  # Normalize\n",
    "\n",
    "            # Generate face embedding\n",
    "            with torch.no_grad():\n",
    "                new_embedding = facenet(face_roi)\n",
    "\n",
    "            # Convert embedding to NumPy array\n",
    "            new_embedding = new_embedding.squeeze().numpy()\n",
    "\n",
    "            # Function to recognize face\n",
    "            def recognize_face(new_embedding, known_faces, threshold=0.6):\n",
    "                min_distance = float(\"inf\")\n",
    "                best_match = \"Unknown\"\n",
    "\n",
    "                for name, stored_embedding in known_faces.items():\n",
    "                    stored_embedding = stored_embedding.squeeze()  # Ensure 1-D\n",
    "                    distance = cosine(new_embedding, stored_embedding)\n",
    "\n",
    "                    if distance < min_distance and distance < threshold:\n",
    "                        min_distance = distance\n",
    "                        best_match = name\n",
    "\n",
    "                return best_match\n",
    "\n",
    "            # Get recognized name\n",
    "            recognized_name = recognize_face(new_embedding, known_faces)\n",
    "\n",
    "            # Display name on screen\n",
    "            if recognized_name != \"Unknown\":\n",
    "                cv2.putText(frame, f\"Hi {recognized_name}!\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3197c-8eb2-4c98-9660-1bcd52d745a8",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Previous Attempts\n",
    "The cells below show earlier versions of the code, like  the progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb30c184-732b-4b6f-9cb9-09e9bcdd3b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107M/107M [00:02<00:00, 48.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FaceNet Model Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "print(\"FaceNet Model Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3b531a6-b178-4340-a59b-00b8f2f9bd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Boxes: [[368.26373291015625 527.827880859375 1028.166748046875 1398.841064453125]]\n",
      "Probabilities: [0.999993085861206]\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize MTCNN \n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Load an image\n",
    "image_path = r\"C:\\Users\\Yathish\\Internship\\cv\\person1.jpeg\"  # Change this to an actual image path\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Detect faces\n",
    "boxes, probs = mtcnn.detect(img)\n",
    "\n",
    "# Print detected face boxes and probabilities\n",
    "print(\"Face Boxes:\", boxes)\n",
    "print(\"Probabilities:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "210d96d9-9a65-4ba7-93fd-d73d19bb9b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Embedding: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# Load FaceNet model\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Extract face embedding\n",
    "img_cropped = mtcnn(img)  \n",
    "if img_cropped is not None:\n",
    "    img_cropped = img_cropped.to(device)\n",
    "    embedding = model(img_cropped.unsqueeze(0))  \n",
    "    print(\"Face Embedding:\", embedding.shape)\n",
    "else:\n",
    "    print(\"No face detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f342558-a6f1-423e-b18b-f2e56bc765da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face embedding stored successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "known_faces = {}\n",
    "\n",
    "\n",
    "name = \"John_Doe\"  \n",
    "\n",
    "# Save the embedding \n",
    "known_faces[name] = embedding.detach().numpy()\n",
    "\n",
    "\n",
    "np.save(\"known_faces.npy\", known_faces)\n",
    "\n",
    "print(\"Face embedding stored successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e316a1f-28c1-4039-bb17-05b729175f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded known faces: dict_keys(['John_Doe'])\n",
      "Recognized: John_Doe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load stored embeddings\n",
    "try:\n",
    "    known_faces = np.load(\"known_faces.npy\", allow_pickle=True).item()\n",
    "    print(\"Loaded known faces:\", known_faces.keys())\n",
    "except FileNotFoundError:\n",
    "    print(\"No known faces found! Please store embeddings first.\")\n",
    "    known_faces = {}\n",
    "\n",
    "\n",
    "new_face_embedding = embedding.detach().numpy() \n",
    "\n",
    "# Compare with stored embeddings\n",
    "def recognize_face(new_embedding, known_faces, threshold=0.6):\n",
    "    if isinstance(new_embedding, torch.Tensor):\n",
    "        new_embedding = new_embedding.squeeze().cpu().numpy()  \n",
    "    else:\n",
    "        new_embedding = new_embedding.squeeze()  \n",
    "\n",
    "    min_distance = float(\"inf\")\n",
    "    best_match = None\n",
    "\n",
    "    for name, stored_embedding in known_faces.items():\n",
    "        stored_embedding = stored_embedding.squeeze()  # Ensure it's 1-D\n",
    "        distance = cosine(new_embedding, stored_embedding)\n",
    "\n",
    "        if distance < min_distance and distance < threshold:\n",
    "            min_distance = distance\n",
    "            best_match = name\n",
    "\n",
    "    return best_match if best_match else \"Unknown\"\n",
    "\n",
    "\n",
    "\n",
    "# Recognize the face\n",
    "recognized_name = recognize_face(new_face_embedding, known_faces)\n",
    "print(f\"Recognized: {recognized_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "146f0f5c-f513-4290-a64e-493488c81236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face embedding for Yathish stored successfully!\n",
      "Loaded known faces: dict_keys(['Yathish'])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Initialize MTCNN\n",
    "mtcnn = MTCNN()\n",
    "facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Dictionary to store embeddings\n",
    "known_faces = {}\n",
    "\n",
    "\n",
    "image_path = r\"C:\\Users\\Yathish\\cv\\person1.jpeg\" \n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Detect and extract face from image\n",
    "img_cropped = mtcnn(img)\n",
    "\n",
    "if img_cropped is not None:\n",
    "    img_cropped = img_cropped.unsqueeze(0)  \n",
    "    with torch.no_grad():\n",
    "        embedding = facenet(img_cropped)  # Generate embedding\n",
    "\n",
    "    # Store embedding with name \"Yathish\"\n",
    "    known_faces[\"Yathish\"] = embedding.numpy()\n",
    "\n",
    "    \n",
    "    np.save(\"known_faces.npy\", known_faces)\n",
    "    print(\"Face embedding for Yathish stored successfully!\")\n",
    "else:\n",
    "    print(\"No face detected in the provided image.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "try:\n",
    "    known_faces = np.load(\"known_faces.npy\", allow_pickle=True).item()\n",
    "    print(\"Loaded known faces:\", known_faces.keys())\n",
    "except FileNotFoundError:\n",
    "    print(\"No stored faces found! Please store embeddings first.\")\n",
    "    exit()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame)\n",
    "\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            x, y, x2, y2 = map(int, box)  # Convert bounding box coordinates to integers\n",
    "\n",
    "            # Ensure valid bounding box coordinates\n",
    "            h, w, _ = frame.shape  # Get frame dimensions\n",
    "            x = max(0, x)\n",
    "            y = max(0, y)\n",
    "            x2 = min(w, x2)\n",
    "            y2 = min(h, y2)\n",
    "\n",
    "            # Extract face ROI safely\n",
    "            face_roi = frame[y:y2, x:x2]\n",
    "\n",
    "            \n",
    "            if face_roi.size == 0:\n",
    "                continue  \n",
    "\n",
    "            face_roi = cv2.resize(face_roi, (160, 160))  # Resize for FaceNet\n",
    "\n",
    "            # Convert to tensor for FaceNet\n",
    "            face_roi = np.transpose(face_roi, (2, 0, 1))  # Change to (C, H, W)\n",
    "            face_roi = torch.tensor(face_roi, dtype=torch.float32).unsqueeze(0) / 255.0  # Normalize\n",
    "\n",
    "            # Generate face embedding\n",
    "            with torch.no_grad():\n",
    "                new_embedding = facenet(face_roi)\n",
    "\n",
    "            # Convert embedding to NumPy array\n",
    "            new_embedding = new_embedding.squeeze().numpy()\n",
    "\n",
    "            # Recognize face\n",
    "            def recognize_face(new_embedding, known_faces, threshold=0.6):\n",
    "                min_distance = float(\"inf\")\n",
    "                best_match = \"Unknown\"\n",
    "\n",
    "                for name, stored_embedding in known_faces.items():\n",
    "                    stored_embedding = stored_embedding.squeeze()  # Ensure 1-D\n",
    "                    distance = cosine(new_embedding, stored_embedding)\n",
    "\n",
    "                    if distance < min_distance and distance < threshold:\n",
    "                        min_distance = distance\n",
    "                        best_match = name\n",
    "\n",
    "                return best_match\n",
    "\n",
    "            # Get recognized name\n",
    "            recognized_name = recognize_face(new_embedding, known_faces)\n",
    "\n",
    "            # Display name on screen\n",
    "            if recognized_name == \"Yathish\":\n",
    "                cv2.putText(frame, \"Hi Yathish!\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de405f35-d398-41e0-801b-cea96614bc69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (face_recog_env)",
   "language": "python",
   "name": "face_recog_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
